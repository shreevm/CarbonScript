# -*- coding: utf-8 -*-
"""VC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aj7NfQdLeNBEWMVJ5TBXrMQ9QMfCsYev
"""

!pip install transformers
!pip install huggingface_hub
!pip install datasets
!pip install gradio

import pickle
from transformers import AutoProcessor, AutoModelForImageTextToText, Trainer, TrainingArguments
from datasets import load_dataset
import torch
import gradio as gr

from google.colab import drive
drive.mount('/content/drive')

import os
from huggingface_hub import login
from google.colab import userdata

# Fetch your Hugging Face API key from the secret
hug_api_key = userdata.get('HF_TOKEN')

# Log in with the API key
login(hug_api_key)

import tensorflow as tf
print(tf.__version__)

import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))

from datasets import load_dataset

# Load dataset (using Hugging Face dataset)
dataset = load_dataset("philschmid/amazon-product-descriptions-vlm")

# Split dataset into training and evaluation sets (80% train, 20% eval)
split_dataset = dataset["train"].train_test_split(test_size=0.2)

train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

# Define the processor and model
# Load model directly
from transformers import AutoProcessor, AutoModelForImageTextToText

processor = AutoProcessor.from_pretrained("meta-llama/Llama-3.2-11B-Vision-Instruct")
model = AutoModelForImageTextToText.from_pretrained("meta-llama/Llama-3.2-11B-Vision-Instruct")

class ProductDescriptionDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, processor, max_len=128):
        self.dataset = dataset
        self.processor = processor
        self.max_len = max_len

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]

        # Concatenate multiple columns into a single text string
        text = f"Product Name: {item['Product Name']} | "
        text += f"Category: {item['Category']} | "
        text += f"Product Specifications: {item['Product Specification']} | "
        text += f"About Product: {item['About Product']} | "
        text += f"Description: {item['description']}"

        # Load and preprocess the image
        image = item['image']
        image_inputs = self.processor(images=image, return_tensors="pt")

        # Tokenize the concatenated description
        text_inputs = self.processor.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt",
        )

        return {
            'image': image_inputs['pixel_values'].squeeze(0),
            'text': text_inputs['input_ids'].squeeze(0),
        }

from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",  # Output directory for saved model checkpoints
    evaluation_strategy="steps",
    eval_steps=100,  # Number of steps between evaluations
    save_steps=200,  # Save the model after every 200 steps
    logging_steps=50,  # Log metrics every 50 steps
    per_device_train_batch_size=8,  # Batch size for training
    per_device_eval_batch_size=8,  # Batch size for evaluation
    num_train_epochs=6,  # Number of training epochs
    save_total_limit=2,  # Limit the number of saved checkpoints
    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available
    report_to="none",  # Do not report to external platforms like TensorBoard
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # Training dataset
    eval_dataset=eval_dataset,  # validation dataset
    tokenizer=processor,  # Tokenizer to process the text
)
# to train without validation dataset
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,  # Training dataset (all data)
#     tokenizer=processor,  # Tokenizer to process the text
# )

# Start training
trainer.train()

# Save the  model and processor

model_path = '/content/drive/MyDrive/fine_tuned_model.pkl.pkl'  # Change the path as needed
joblib.dump(model, model_path)

# Save the processor as well
processor_path = '/content/drive/MyDrivefine_tuned_processor.pkl'
joblib.dump(processor, processor_path)

print("Model and processor saved to Google Drive!")
# Save the fine-tuned model and processor in pickle format

# Load the prompt from a text file
with open("/content/drive/MyDrive/Colab Notebooks/prompt_template.txt", "r") as file:
    prompt_template = file.read()

# Use the prompt from the file in the image description generation
def generate_description_with_prompt(image, prompt_file="prompt.txt"):
    """
    Generates product name and description using a model with prompt from the prompt.txt file.
    """
    # Read the prompt from the file
    start_time = time.time()  # Start latency timer
    prompt = read_prompt_from_file(prompt_file)

    # Preprocess the image
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Combine the prompt with the image features
    input_text = prompt + " | "  # Combine the prompt with the image (optional, depends on your prompt structure)

    # Tokenize the combined input (prompt + image)
    text_inputs = processor.tokenizer(
        input_text,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt",
    )

    # Generate the description based on the prompt and image
    with torch.no_grad():
        output = model.generate(**inputs, max_length=50)
        generated_caption = processor.decode(output[0], skip_special_tokens=True)

    # Extract product name and description from the generated text
    if "|" in generated_caption:
        parts = generated_caption.split("|")
        product_name = parts[0].strip()
        product_description = parts[1].strip() if len(parts) > 1 else "Description Not Detected"
    else:
        product_name = "Product Name Not Detected"
        product_description = generated_caption.strip()
    # Calculate latency
    latency = time.time() - start_time

    return product_name, product_description,latency

# Gradio Interface setup
def generate_and_evaluate(image, ground_truth):
    product_name, product_description = generate_description_with_prompt(image)
    accuracy = compute_accuracy(product_description, ground_truth)
    return product_name, product_description, accuracy

def compute_accuracy(generated_description, ground_truth):
    if not ground_truth:
        return "No Ground Truth Provided"
    generated_words = set(generated_description.lower().split())
    ground_truth_words = set(ground_truth.lower().split())
    overlap = len(generated_words & ground_truth_words)
    total = len(ground_truth_words)
    return f"{round((overlap / total) * 100, 2)}%"

def collect_feedback(product_name, product_description, feedback):
    with open("feedback.txt", "a") as feedback_file:
        feedback_file.write(
            f"Product Name: {product_name}\nDescription: {product_description}\nFeedback: {feedback}\n{'-'*50}\n"
        )
    return "Thank you for your feedback!"

# Gradio interface
with gr.Blocks() as demo:
    gr.HTML("<h1>VisonClarity</h1>")
    gr.Markdown(
        "Upload an image, and optionally provide ground truth for accuracy evaluation. "
        "You can also provide feedback on the generated outputs."
    )

    with gr.Tab("Generate Description"):
        product_image = gr.Image(label="Upload Product Image")

        product_name = gr.Textbox(label="Generated Product Name", interactive=False)
        product_description = gr.Textbox(label="Generated Product Description", interactive=False)
        latency=gr.Textbox(label="Latency",interactive=False)
        accuracy_score = gr.Textbox(label="Accuracy", interactive=False)
        generate_button = gr.Button("Generate and Evaluate")
        generate_button.click(
            generate_and_evaluate,
            inputs=[product_image],
            outputs=[product_name, product_description, latency, accuracy_score],
        )

    with gr.Tab("Provide Feedback"):
        user_feedback = gr.Textbox(label="Your Feedback", placeholder="What do you think about the generated results?")
        submit_button = gr.Button("Submit Feedback")
        submit_button.click(
            collect_feedback,
            inputs=[product_name, product_description, user_feedback],
            outputs="text",
        )

demo.launch(share=True, debug=True)

import time
import torch

# Function to generate both product name and product description
def generate_description(image):
    """
    Generates a product name and description from the provided image.

    Args:
        image: The input product image.

    Returns:
        A tuple containing:
            - Product name
            - Product description
            - Latency in seconds
    """
    start_time = time.time()  # Start latency timer

    # Preprocess the image for the model
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Generate a caption/description for the image
    with torch.no_grad():  # No need to compute gradients for inference
        output = model.generate(**inputs, max_length=50)
        generated_caption = processor.decode(output[0], skip_special_tokens=True)

    # Parse the generated caption to extract product name and description
    if "|" in generated_caption:
        # Assume the generated caption format is "Product Name | Product Description"
        parts = generated_caption.split("|")
        product_name = parts[0].strip()  # The first part contains the product name
        product_description = parts[1].strip() if len(parts) > 1 else "Description Not Detected"
    else:
        # Handle cases where no separator is present
        product_name = "Product Name Not Detected"
        product_description = generated_caption.strip()

    # Calculate latency
    latency = time.time() - start_time

    return product_name, product_description, latency

# Function to compute accuracy
def compute_accuracy(generated_description, ground_truth):
    if not ground_truth:
        return "No Ground Truth Provided"
    generated_words = set(generated_description.lower().split())
    ground_truth_words = set(ground_truth.lower().split())
    overlap = len(generated_words & ground_truth_words)
    total = len(ground_truth_words)
    return f"{round((overlap / total) * 100, 2)}%"

# Define Gradio interface
def generate_and_evaluate(image, ground_truth):
    product_name, product_description = generate_product_description(image)
    accuracy = compute_accuracy(product_description, ground_truth)
    return product_name, product_description, accuracy

def collect_feedback(product_name, product_description, feedback):
    with open("feedback.txt", "a") as feedback_file:
        feedback_file.write(
            f"Product Name: {product_name}\nDescription: {product_description}\nFeedback: {feedback}\n{'-'*50}\n"
        )
    return "Thank you for your feedback!"

# Create Gradio Blocks interface
with gr.Blocks() as demo:
    gr.HTML("<h1>VisonClarity</h1>")
    gr.Markdown(
        "Upload an image, and optionally provide ground truth for accuracy evaluation. "
        "You can also provide feedback on the generated outputs."
    )

    with gr.Tab("Generate Description"):
        product_image = gr.Image(label="Upload Product Image")
        ground_truth = gr.Textbox(
            label="Ground Truth (Optional)",
            placeholder="Provide ground truth description for accuracy calculation",
        )
        product_name = gr.Textbox(label="Generated Product Name", interactive=False)
        product_description = gr.Textbox(label="Generated Product Description", interactive=False)
        accuracy_score = gr.Textbox(label="Accuracy", interactive=False)
        generate_button = gr.Button("Generate and Evaluate")
        generate_button.click(
            generate_and_evaluate,
            inputs=[product_image, ground_truth],
            outputs=[product_name, product_description, accuracy_score],
        )

    with gr.Tab("Provide Feedback"):
        user_feedback = gr.Textbox(label="Your Feedback", placeholder="What do you think about the generated results?")
        submit_button = gr.Button("Submit Feedback")
        submit_button.click(
            collect_feedback,
            inputs=[product_name, product_description, user_feedback],
            outputs="text",
        )

demo.launch(share=True,debug=True)

# Preprocess the dataset for training
def preprocess_function(examples):
    images = dataset["image"]  # Assuming "image" contains the image paths or PIL Images
    texts = examples["Product Name"]
    inputs = processor(images=images, text=texts, return_tensors="pt", padding=True)
    return {
        "input_ids": inputs.input_ids.squeeze(0),
        "attention_mask": inputs.attention_mask.squeeze(0),
        "pixel_values": inputs.pixel_values.squeeze(0),
        "labels": inputs.input_ids.squeeze(0),  # Align labels with input IDs
    }

# Map the preprocessing function to the dataset
processed_dataset = dataset.map(preprocess_function, batched=True)



