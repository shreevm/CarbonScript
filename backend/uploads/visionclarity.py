# -*- coding: utf-8 -*-
"""VisionClarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zPMocxQnehsp1oi_gCKB8mK-9vZa7EyH
"""

!pip install transformers
!pip install huggingface_hub
!pip install datasets
!pip install gradio

import torch
import joblib
from transformers import BlipProcessor, BlipForConditionalGeneration
from torch.utils.data import DataLoader
from torch.optim import AdamW
from datasets import load_dataset
from tqdm import tqdm
import random
from torch import nn
import gradio as gr
!huggingface-cli whoami

import os
from huggingface_hub import login
from google.colab import userdata

# Fetch your Hugging Face API key from the secret
hug_api_key = userdata.get('HF_TOKEN')

# Log in with the API key
login(hug_api_key)



# Load dataset from Hugging Face
datasets = load_dataset("philschmid/amazon-product-descriptions-vlm")

from google.colab import drive
drive.mount('/content/drive')

# Load the BLIP processor and model (image captioning)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

# Define a custom dataset to load the image-text pairs from the dataset
class ProductDescriptionDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, processor, max_len=128):
        self.dataset = dataset
        self.processor = processor
        self.max_len = max_len

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]

        # Concatenate the product description fields
        text = f"Product Description: {item['Product Name']} | "
        text += f"Category: {item['Category']} | "
        text += f"Product Specifications: {item['Product Specification']} | "
        text += f"Technical Details: {item['Technical Details']} | "
        text += f"About Product: {item['About Product']} | "
        text += f"Description: {item['description']}"

        # Load and preprocess the image
        image = item['image']
        image_inputs = self.processor(images=image, return_tensors="pt")

        # Tokenize the description
        text_inputs = self.processor.tokenizer(text, padding="max_length", truncation=True, max_length=self.max_len, return_tensors="pt")

        return {
            'image': image_inputs['pixel_values'].squeeze(0),
            'text': text_inputs['input_ids'].squeeze(0),
        }

# Create the dataset and dataloader
train_dataset = ProductDescriptionDataset(datasets['train'], processor)
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Optimizer and loss
optimizer = AdamW(model.parameters(), lr=5e-6)

# Fine-tune the BLIP model with the correct descriptions
epochs = 5
for epoch in range(epochs):
    total_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
        image_inputs = batch['image'].to(device)
        text_inputs = batch['text'].to(device)

        outputs = model(input_ids=text_inputs, pixel_values=image_inputs, labels=text_inputs)
        loss = outputs.loss
        total_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")

# Save the BLIP model and processor

model_path = '/content/drive/MyDrive/blip_trained_model_3_cpu.pkl'  # Change the path as needed
joblib.dump(model, model_path)

# Save the processor as well
processor_path = '/content/drive/MyDrive/blip_processor_3_cpu.pkl'
joblib.dump(processor, processor_path)

print("Model and processor saved to Google Drive!")

import gradio as gr
import torch
import joblib
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# Load the trained model and processor
processor = joblib.load('/content/drive/MyDrive/blip_processor_3_cpu.pkl')  # Load trained model
model = joblib.load('/content/drive/MyDrive/blip_trained_model_3_cpu.pkl')  # Load processor
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()  # Set model to evaluation mode



# Function to generate a caption and simulate product details
def generate_description(image):
    # Preprocess the image for the model
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Generate a description for the image
    with torch.no_grad():  # No need to compute gradients for inference
        output = model.generate(**inputs, max_length=50)
        generated_caption = processor.decode(output[0], skip_special_tokens=True)

    if "|" in generated_caption:
        parts = generated_caption.split("|")
        product_description = parts[0].strip()  # The first part contains the main description
    else:
        product_description = generated_caption.strip()

    # Return the description without "Product Name"
    return f"Product Description: {product_description}"

# Create the Gradio interface
iface = gr.Interface(
    fn=generate_description,
    inputs=gr.Image(type="pil"),
    outputs="text",
    live=True,
)

# Launch the Gradio interface
iface.launch(debug=True)

import gradio as gr
import torch
import joblib
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# Load the trained model and processor
processor = joblib.load('/content/drive/MyDrive/blip_processor_3.pkl')  # Load trained model
model = joblib.load('/content/drive/MyDrive/blip_trained_model_3.pkl')  # Load processor
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()  # Set model to evaluation mode



# Function to generate a caption and simulate product details
def generate_description(image):
    # Preprocess the image for the model
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Generate a description for the image
    with torch.no_grad():  # No need to compute gradients for inference
        output = model.generate(**inputs, max_length=50)
        generated_caption = processor.decode(output[0], skip_special_tokens=True)


    parts = generated_caption.split("|")
    product_name = parts[0].strip() if len(parts) > 0 else "Product name not available."
    product_description = parts[1].strip() if len(parts) > 1 else "Description not available."

    # Return only the product name and description
    #return f"Product Name: {product_name}\n\nProduct Description:** {product_description}"
    return f"Product Description: {product_name}"

# Create the Gradio interface
iface = gr.Interface(
    fn=generate_description,
    inputs=gr.Image(type="pil"),  # Input type: image (PIL format)
    outputs="text",  # Output type: single text block
    live=True,  # Update output in real-time
)

# Launch the Gradio interface
iface.launch(debug=True)

# Define a GAN-based model for introducing incorrect descriptions

class GAN(nn.Module):
    def __init__(self):
        super(GAN, self).__init__()
        self.generator = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # Output size = 1 (incorrect description probability)
        )
        self.discriminator = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # Output size = 1 (real/fake classification)
        )

    def forward(self, x):
        # Forward pass for generator and discriminator
        fake = self.generator(x)
        disc_output = self.discriminator(fake)
        return fake, disc_output

# Initialize the GAN model
gan_model = GAN().to(device)

# Loss functions and optimizers for GAN
criterion = nn.BCEWithLogitsLoss()
optimizer_gan = AdamW(gan_model.parameters(), lr=1e-5)

# Now train the GAN with corrupted descriptions
def corrupt_description(text):
    words = text.split(' ')
    random.shuffle(words)  # Shuffle words to simulate a corrupted description
    return ' '.join(words)

# Train the GAN model
epochs = 3
for epoch in range(epochs):
    total_loss = 0
    for batch in tqdm(train_dataloader, desc=f"GAN Epoch {epoch+1}/{epochs}"):
        image_inputs = batch['image'].to(device)
        text_inputs = batch['text'].to(device)

        # Get original description embeddings
        #image_embeddings = model.get_text_features(pixel_values=image_inputs)
        text_embeddings = model.get_text_features(input_ids=text_inputs)

        # Corrupt descriptions using the GAN
        corrupted_descriptions = [corrupt_description(desc) for desc in batch['text']]
        corrupted_inputs = processor.tokenizer(corrupted_descriptions, padding=True, truncation=True, return_tensors="pt").to(device)

        # Forward pass through the GAN
        fake, disc_output = gan_model(text_embeddings)

        # Compute loss for discriminator and generator
        disc_loss = criterion(disc_output, torch.ones_like(disc_output).to(device))
        gen_loss = criterion(fake, torch.zeros_like(disc_output).to(device))

        # Backpropagate the loss
        optimizer_gan.zero_grad()
        disc_loss.backward()
        gen_loss.backward()
        optimizer_gan.step()

        total_loss += disc_loss.item() + gen_loss.item()

    avg_loss = total_loss / len(train_dataloader)
    print(f"GAN Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")

# Save the model with GAN

model_path = '/content/drive/MyDrive/GAN_model_1.pkl'  # Change the path as needed
joblib.dump(model, model_path)

# Save the processor as well
processor_path = '/content/drive/MyDrive/GAN_processor_1.pkl'
joblib.dump(processor, processor_path)

print("Model and processor saved to Google Drive!")

# Gradio interface for real-time prediction
def generate_description(image):
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Generate caption
    with torch.no_grad():
        output = model.generate(**inputs, max_length=50)
        generated_caption = processor.decode(output[0], skip_special_tokens=True)

    # Simulated extraction of product name and description
    parts = generated_caption.split("|")
    product_name = parts[0].strip() if len(parts) > 0 else "Product name not available."
    product_description = parts[1].strip() if len(parts) > 1 else "Description not available."

    return f"**Product Name:** {product_name}\n\n**Product Description:** {product_description}"

# Create the Gradio interface
iface = gr.Interface(
    fn=generate_description,
    inputs=gr.Image(type="pil"),
    outputs="text",
    live=True,
)

# Launch the Gradio interface
iface.launch()

import gradio as gr
import torch
import joblib
import time
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
from sklearn.metrics import accuracy_score

# Load the trained model and processor
processor = joblib.load('/content/drive/MyDrive/blip_processor_3.pkl')  # Load processor
model = joblib.load('/content/drive/MyDrive/blip_trained_model_3.pkl')  # Load trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()  # Set model to evaluation mode

# Function to generate a description
def generate_description(image):
    start_time = time.time()  # Start latency timer

    # Preprocess the image for the model
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Generate a description for the image
    with torch.no_grad():  # No need to compute gradients for inference
        output = model.generate(**inputs, max_length=50)
        generated_caption = processor.decode(output[0], skip_special_tokens=True)

    # Simulate product name and description extraction
    parts = generated_caption.split("|")
    description = parts[1].strip() if len(parts) > 1 else "Description not available."

    # Calculate latency
    latency = time.time() - start_time

    return description, latency

# Evaluation metrics WITH ground truth
def evaluate_with_ground_truth(generated_description, ground_truth):
    # Accuracy (Exact Match)
    exact_match = 1 if generated_description.strip().lower() == ground_truth.strip().lower() else 0

    # Jaccard Similarity (Token-based overlap)
    gen_tokens = set(generated_description.lower().split())
    truth_tokens = set(ground_truth.lower().split())
    intersection = gen_tokens.intersection(truth_tokens)
    union = gen_tokens.union(truth_tokens)
    jaccard_similarity = len(intersection) / len(union) if union else 0

    return {
        "Exact Match Accuracy": exact_match,
        "Jaccard Similarity": jaccard_similarity,
    }

# Proxy evaluation metrics WITHOUT ground truth
def evaluate_without_ground_truth(description, expected_keywords):
    # Coherence Check (basic qualitative evaluation)
    coherence_score = 1 if description and len(description.split()) > 5 else 0  # A simple threshold

    # Keyword Matching
    matched_keywords = [kw for kw in expected_keywords if kw.lower() in description.lower()]
    keyword_coverage = len(matched_keywords) / len(expected_keywords) if expected_keywords else 0

    # Relevance (human feedback placeholder)
    relevance_score = "Requires manual review"  # Replace with human evaluation if possible

    return {
        "Coherence Score": coherence_score,
        "Keyword Coverage": keyword_coverage,
        "Relevance": relevance_score,
    }

# Gradio interface function
def process_image(image, ground_truth=None):
    # Generate description and calculate latency
    description, latency = generate_description(image)

    # If ground truth is provided, evaluate with it
    if ground_truth:
        evaluation_with_gt = evaluate_with_ground_truth(description, ground_truth)
        gt_results = (
            f"Exact Match Accuracy: {evaluation_with_gt['Exact Match Accuracy']}\n"
            f"Jaccard Similarity: {evaluation_with_gt['Jaccard Similarity']:.2f}\n"
        )
    else:
        gt_results = "No ground truth provided for this image.\n"

    # Example: Define expected keywords for unseen data (proxy for ground truth)
    expected_keywords = ["camera", "lens", "mirrorless", "zoom"]

    # Evaluate without ground truth
    evaluation_without_gt = evaluate_without_ground_truth(description, expected_keywords)
    no_gt_results = (
        f"Coherence Score: {evaluation_without_gt['Coherence Score']}\n"
        f"Keyword Coverage: {evaluation_without_gt['Keyword Coverage']:.2f}\n"
        f"Relevance: {evaluation_without_gt['Relevance']}\n"
    )

    # Display output
    result = f"Generated Description: {description}\n\n"
    result += f"Latency: {latency:.2f} seconds\n\n"
    result += "Evaluation WITH Ground Truth:\n" + gt_results + "\n"
    result += "Evaluation WITHOUT Ground Truth:\n" + no_gt_results
    return result

# Create the Gradio interface
iface = gr.Interface(
    fn=process_image,
    inputs=[
        gr.Image(type="pil"),  # Input type: image (PIL format)
        gr.Textbox(label="Ground Truth Description (optional)"),  # Optional ground truth
    ],
    outputs="text",  # Output type: text
    live=True,  # Real-time updates
)

# Launch the Gradio app
iface.launch(debug=True)

